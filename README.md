# ğŸ§  Attention Is All You Need â€” NLP Project

A comprehensive implementation and comparison of modern neural network architectures in **Natural Language Processing (NLP)** using **PyTorch** and **HuggingFace Transformers**.

---

## ğŸ” Tasks Covered

- ğŸ—‚ **Text Classification**  
- ğŸ”„ **Sequence-to-Sequence Translation** (e.g., French to English)  
- â“ **Question Answering**  
- âœï¸ **Text Generation**

---

## ğŸ“š Background

### ğŸ§® Recurrent Neural Networks (RNNs)
RNNs process sequential data by maintaining a hidden state that captures temporal patterns. Theyâ€™re effective for language modeling, translation, and text generation.  
**Limitations**: Vanishing gradient problem, poor long-term memory.  
**Improvement**: **Bidirectional RNNs** read sequences both forward and backward to capture richer context.

### ğŸ” Encoder-Decoder Architecture
A classical **Seq2Seq** approach using RNNs:
- **Encoder**: Compresses input into a fixed context vector.
- **Decoder**: Generates the output from the context.

![](/Images/attention_research_1.webp)

### âš¡ Transformers (Vaswani et al., 2017)
Replaces recurrence with **self-attention** mechanisms for parallelism and long-range dependency modeling.  
Key components:
- Multi-head self-attention  
- Positional encoding  
- Scalable parallel training  

### ğŸ§  BERT (Bidirectional Encoder Representations from Transformers)
A deep, pre-trained Transformer encoder using:
- **Masked Language Modeling (MLM)**
- **Next Sentence Prediction (NSP)**  
It generates **context-aware embeddings** fine-tuned for various NLP tasks.

![](/Images/Bert GPT.jpeg)

---

## ğŸ“‚ Project Structure

```bash
attention-is-all-you-need/
â”‚
â”œâ”€â”€ Classify text with BERT/                       # Fine-tune BERT for classification
â”œâ”€â”€ Generating Text Using a Transformer Decoder-Only Model/  # GPT-style generation
â”œâ”€â”€ Images/                                        # Visualizations
â”œâ”€â”€ Machine Translation French to English/         # Seq2Seq & Transformer-based translation
â”œâ”€â”€ Question-Answering with a Multi-Layer Bidirectional RNN/ # QA with Bi-RNN
â”œâ”€â”€ Simple RNN Encode-Decoder for Translation/     # RNN-based translation
â”œâ”€â”€ Text Generation with RNN/                      # Language modeling with RNN
â”œâ”€â”€ Attention_is_all_you_need.pdf                  # Original Transformer paper
â””â”€â”€ README.md                                      # Project overview

---

## ğŸ§ª Components and Descriptions

### 1. **Classify text with BERT/**
Fine-tunes a pre-trained BERT model for binary/multi-class text classification. Includes:
- Tokenization & preprocessing
- Training loop
- Evaluation: Accuracy, F1-score

### 2. **Generating Text Using a Transformer Decoder-Only Model/**
Implements a GPT-style **decoder-only Transformer** for autoregressive text generation.

### 3. **Machine Translation French to English/**
Compares:
- RNN-based Seq2Seq model  
- Transformer-based model  
Evaluation: **BLEU score**

### 4. **Question-Answering with a Multi-Layer Bidirectional RNN/**
Trains a **Bi-RNN** to extract answers from given passages â€” an early-style QA system.

### 5. **Simple RNN Encode-Decoder for Translation/**
Implements a **GRU/LSTM-based** Seq2Seq model with:
- Teacher forcing
- BLEU score evaluation

### 6. **Text Generation with RNN/**
Trains a **character-level or word-level RNN** for creative text generation.  
Includes temperature-based sampling for more controlled generation output.

---

## ğŸ“˜ Paper Reference

> **Attention Is All You Need**  
> *Vaswani et al., 2017*  
> This project is heavily inspired by this landmark work in NLP.

---

## âš™ï¸ Requirements

Install all dependencies with:

```bash
pip install -r requirements.txt

