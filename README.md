# ğŸ§  Attention Is All You Need â€” NLP Project

A comprehensive implementation and comparison of modern neural network architectures in **Natural Language Processing (NLP)** using **PyTorch** and **HuggingFace Transformers**.

---

## ğŸ” Tasks Covered

- ğŸ—‚ **Text Classification**  
- ğŸ”„ **Sequence-to-Sequence Translation** (e.g., French to English)  
- â“ **Question Answering**  
- âœï¸ **Text Generation**

---

## ğŸ“š Background

### ğŸ§® Recurrent Neural Networks (RNNs)
RNNs process sequential data by maintaining a hidden state that captures temporal patterns. Theyâ€™re effective for language modeling, translation, and text generation.  
**Limitations**: Vanishing gradient problem, poor long-term memory  
**Improvement**: **Bidirectional RNNs** read sequences both forward and backward to capture richer context.

### ğŸ” Encoder-Decoder Architecture
A classical **Seq2Seq** approach using RNNs:
- **Encoder**: Compresses input into a fixed context vector  
- **Decoder**: Generates the output from the context

![Encoder-Decoder](/Images/attention_research_1.webp)

### âš¡ Transformers (Vaswani et al., 2017)
Replaces recurrence with **self-attention** mechanisms for parallelism and modeling long-range dependencies.  
Key components:
- Multi-head self-attention  
- Positional encoding  
- Scalable parallel training  

### ğŸ§  BERT (Bidirectional Encoder Representations from Transformers)
A deep, pre-trained Transformer encoder using:
- **Masked Language Modeling (MLM)**  
- **Next Sentence Prediction (NSP)**  
It generates **context-aware embeddings** fine-tuned for various NLP tasks.

![BERT and GPT](/Images/Bert%20GPT.jpeg)

---

## ğŸ“‚ Project Structure

```bash
attention-is-all-you-need/
â”‚
â”œâ”€â”€ Classify text with BERT/                       # Fine-tune BERT for classification
â”œâ”€â”€ Generating Text Using a Transformer Decoder-Only Model/  # GPT-style generation
â”œâ”€â”€ Images/                                        # Visualizations
â”œâ”€â”€ Machine Translation French to English/         # Seq2Seq & Transformer-based translation
â”œâ”€â”€ Question-Answering with a Multi-Layer Bidirectional RNN/ # QA with Bi-RNN
â”œâ”€â”€ Simple RNN Encode-Decoder for Translation/     # RNN-based translation
â”œâ”€â”€ Text Generation with RNN/                      # Language modeling with RNN
â”œâ”€â”€ Attention_is_all_you_need.pdf                  # Original Transformer paper
â””â”€â”€ README.md                                      # Project overview
